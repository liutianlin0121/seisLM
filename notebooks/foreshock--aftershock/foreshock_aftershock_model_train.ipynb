{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from seisLM.data_pipeline import foreshock_aftershock_dataloaders as loaders\n",
    "from seisLM.model.task_specific import foreshock_aftershock_models as models\n",
    "\n",
    "dataloaders = loaders.prepare_foreshock_aftershock_dataloaders(\n",
    "  num_classes=2,\n",
    "  batch_size=32,\n",
    "  component_order='ZNE',\n",
    "  event_split_method='temporal'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveforms, batches = next(iter(dataloaders['train']))\n",
    "# print(waveforms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seisLM.model.foundation import pretrained_models\n",
    "import json\n",
    "import ml_collections\n",
    "from seisLM.model.task_specific.foreshock_aftershock_models import Wav2Vec2ForSequenceClassification\n",
    "\n",
    "config_path = '/scicore/home/dokman0000/liu0003/projects/seisLM/seisLM/configs/foreshock_aftershock/seisLM_shock_classifier.json'\n",
    "\n",
    "\n",
    "with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "  model_config = json.load(f)\n",
    "model_config = ml_collections.ConfigDict(model_config).model_args\n",
    "\n",
    "\n",
    "pretrained_model = pretrained_models.LitMultiDimWav2Vec2.load_from_checkpoint(\n",
    "  model_config.pretrained_ckpt_path\n",
    ").model\n",
    "\n",
    "## TODO: temp fix for the config issue\n",
    "\n",
    "new_config = pretrained_model.config\n",
    "for key, value in model_config.items():\n",
    "  # new_config[key] = value\n",
    "  setattr(new_config, key, value)\n",
    "\n",
    "model_config = new_config\n",
    "model = Wav2Vec2ForSequenceClassification(model_config)\n",
    "\n",
    "model.wav2vec2.load_state_dict(\n",
    "  pretrained_model.wav2vec2.state_dict()\n",
    ")\n",
    "model.freeze_feature_encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      3\u001b[0m waveforms, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveforms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/seisbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/seisbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloaders['train']))\n",
    "\n",
    "waveforms, labels = batch\n",
    "model(waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.3579e-03, -1.0114e-02, -3.6488e-02,  2.4164e-02, -1.7462e-02,\n",
       "          1.1483e-02,  7.0830e-02, -5.0917e-03],\n",
       "        [-3.7748e-02,  2.3717e-02, -3.4523e-02,  2.1795e-02,  2.7531e-02,\n",
       "         -9.7431e-03,  6.2395e-02,  2.6684e-02],\n",
       "        [-5.0361e-02, -1.2497e-02, -1.1258e-02,  1.2768e-02,  8.0464e-03,\n",
       "          3.8091e-03,  6.9835e-02,  1.2279e-02],\n",
       "        [-4.4289e-02, -3.7185e-02, -1.9467e-02,  3.1801e-02, -3.0180e-02,\n",
       "          1.4837e-02,  3.5877e-02,  3.7556e-03],\n",
       "        [-4.2501e-02, -1.4874e-02, -5.1696e-02,  1.4387e-02,  1.7219e-02,\n",
       "          1.9677e-03,  3.0164e-02,  1.6754e-02],\n",
       "        [-4.3103e-02,  6.9611e-02,  1.4611e-03, -4.2986e-03,  1.5814e-02,\n",
       "          2.3448e-02,  1.0271e-01,  1.9547e-02],\n",
       "        [-4.3008e-03, -1.9803e-02, -4.1481e-02,  2.9769e-02, -1.1264e-03,\n",
       "         -3.7753e-03,  4.2563e-02, -1.4799e-02],\n",
       "        [-3.8222e-03, -1.5987e-02, -3.6267e-02,  1.3079e-02,  1.5272e-03,\n",
       "         -1.0030e-02,  2.9016e-02, -2.7978e-02],\n",
       "        [ 6.4797e-03,  1.7751e-02, -1.6512e-02,  2.8970e-02, -6.1926e-03,\n",
       "          1.1010e-02,  3.4946e-02, -5.4644e-03],\n",
       "        [-2.1110e-02,  3.7798e-02, -4.3199e-02,  4.4619e-02,  4.1589e-02,\n",
       "         -1.4870e-02,  4.2301e-02,  3.2170e-02],\n",
       "        [-2.6449e-02, -4.1417e-02, -1.7453e-03,  2.2047e-02, -3.8540e-02,\n",
       "         -3.7017e-04, -2.6162e-02,  1.3501e-02],\n",
       "        [ 2.1174e-03,  4.1478e-03, -3.1521e-02,  4.4606e-02,  6.8108e-03,\n",
       "         -1.0303e-03,  6.6332e-02, -1.0000e-02],\n",
       "        [-3.0975e-02,  9.6313e-03, -6.4847e-03,  3.1070e-02, -4.9253e-04,\n",
       "          7.0946e-03,  7.1884e-02,  1.0380e-02],\n",
       "        [-1.7836e-02,  4.5880e-02,  3.0499e-04,  2.4868e-02,  1.4734e-02,\n",
       "         -4.0335e-04,  8.3745e-02,  3.3017e-02],\n",
       "        [-1.6574e-02, -2.2548e-02, -2.4633e-02, -2.0367e-03,  4.9159e-04,\n",
       "          8.8068e-03,  3.7436e-02, -4.6948e-03],\n",
       "        [-2.7003e-02,  1.5464e-03, -3.6628e-02,  1.3748e-02,  7.9781e-03,\n",
       "         -1.1711e-02,  4.4042e-02,  1.2523e-02],\n",
       "        [-2.1758e-02,  3.4510e-02, -4.9725e-02,  4.8999e-02,  2.0230e-02,\n",
       "         -1.4377e-03,  7.8013e-02,  2.9520e-02],\n",
       "        [-5.4221e-02, -7.8193e-03, -4.5104e-02,  3.9829e-02, -1.4821e-03,\n",
       "         -9.2974e-03,  3.3179e-02, -1.8953e-02],\n",
       "        [ 1.7775e-02,  9.8261e-03, -4.5143e-02,  2.6924e-02, -1.0597e-02,\n",
       "         -4.0071e-03,  3.5780e-02, -1.1109e-02],\n",
       "        [-1.3243e-02, -2.1595e-02, -2.2581e-04,  1.7305e-02, -2.7630e-02,\n",
       "          2.1757e-02,  1.8544e-02, -1.5655e-02],\n",
       "        [ 2.7850e-05,  1.7880e-02, -2.6851e-02,  2.9567e-02,  7.0441e-03,\n",
       "          7.1190e-04,  4.8799e-02,  8.0741e-03],\n",
       "        [-3.0332e-02, -2.2610e-02, -4.2711e-02,  3.8738e-02,  1.3058e-02,\n",
       "         -1.3631e-02,  3.0884e-02, -2.2817e-02],\n",
       "        [ 1.5987e-02, -3.8027e-03, -3.6417e-02,  1.9481e-02, -6.7206e-03,\n",
       "         -5.8162e-03,  4.9507e-02, -4.5110e-02],\n",
       "        [ 5.2813e-03,  8.5522e-03, -2.8418e-02,  2.8544e-02, -9.3576e-05,\n",
       "          7.8777e-03,  4.4381e-02, -1.6586e-02],\n",
       "        [-2.7012e-02, -2.4377e-02, -2.0430e-02,  2.7181e-02, -3.1426e-02,\n",
       "          2.6532e-02,  3.1924e-02,  1.6889e-02],\n",
       "        [-5.3842e-02, -2.1230e-02, -3.1604e-02,  2.4127e-02, -1.0493e-03,\n",
       "          2.1829e-02,  1.8216e-02, -1.7829e-03],\n",
       "        [-4.2873e-02,  1.7445e-02, -4.2332e-02,  2.8170e-02,  2.6724e-02,\n",
       "         -1.3769e-02,  5.8776e-02,  1.3637e-02],\n",
       "        [-6.2201e-03, -4.1389e-02,  5.9259e-03,  2.2807e-02, -5.0739e-02,\n",
       "          1.2701e-02,  3.1501e-05, -2.3638e-02],\n",
       "        [-5.6342e-02, -1.5744e-02, -1.0436e-02,  1.4207e-02, -9.2595e-03,\n",
       "          6.7997e-03,  6.0571e-02,  7.6176e-03],\n",
       "        [-2.6837e-02, -4.0808e-02, -1.4202e-02,  3.5431e-02, -3.2262e-02,\n",
       "         -1.6320e-03,  7.3664e-03,  2.5052e-03],\n",
       "        [-1.2790e-02,  1.0326e-02, -3.9365e-03,  1.5239e-02,  1.6558e-03,\n",
       "         -2.9559e-02,  2.6968e-02, -6.3808e-03],\n",
       "        [-1.0871e-02, -2.1407e-02,  2.5782e-04,  1.3433e-02, -1.2177e-02,\n",
       "         -1.4774e-03,  6.1783e-02,  4.5623e-03]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model(input_values=waveforms)\n",
    "\n",
    "# input_values = waveforms\n",
    "# outputs = model.wav2vec2(\n",
    "#     input_values,\n",
    "#     attention_mask=None,\n",
    "#     output_attentions=False,\n",
    "#     output_hidden_states=False,\n",
    "# )\n",
    "\n",
    "# # if self.config.use_weighted_layer_sum:\n",
    "# #     hidden_states = outputs[_HIDDEN_STATES_START_POSITION]\n",
    "# #     hidden_states = torch.stack(hidden_states, dim=1)\n",
    "# #     norm_weights = nn.functional.softmax(self.layer_weights, dim=-1)\n",
    "# #     hidden_states = (hidden_states * norm_weights.view(-1, 1, 1)).sum(dim=1)\n",
    "# # else:\n",
    "# # TODO: implement the weighted layer sum version.\n",
    "\n",
    "# hidden_states = outputs.last_hidden_state\n",
    "# hidden_states = model.projector(hidden_states)\n",
    "# pooled_output = hidden_states.mean(dim=1)\n",
    "# logits = model.classifier(pooled_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForSequenceClassification(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(3, 256, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=256, out_features=240, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          240, 240, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=240, out_features=240, bias=True)\n",
       "            (v_proj): Linear(in_features=240, out_features=240, bias=True)\n",
       "            (q_proj): Linear(in_features=240, out_features=240, bias=True)\n",
       "            (out_proj): Linear(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "            (output_dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): Linear(in_features=240, out_features=1024, bias=True)\n",
       "  (classifier): Linear(in_features=1024, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(wavforms, torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Input shape: (batch_size, 3, 2500) where 3 is the number of channels\n",
    "# and 2500 is the time length\n",
    "model = models.Conv1DShockClassifier(\n",
    "  in_channels=3,\n",
    "  num_classes=2,\n",
    "  num_layers=5,\n",
    "  initial_filters=32,\n",
    "  kernel_size=3,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Conv1DShockClassifier                    [32, 2]                   --\n",
       "├─Sequential: 1-1                        [32, 512, 75]             --\n",
       "│    └─DoubleConvBlock: 2-1              [32, 32, 1248]            --\n",
       "│    │    └─Sequential: 3-1              [32, 32, 1248]            3,488\n",
       "│    └─DoubleConvBlock: 2-2              [32, 64, 622]             --\n",
       "│    │    └─Sequential: 3-2              [32, 64, 622]             18,688\n",
       "│    └─DoubleConvBlock: 2-3              [32, 128, 309]            --\n",
       "│    │    └─Sequential: 3-3              [32, 128, 309]            74,240\n",
       "│    └─DoubleConvBlock: 2-4              [32, 256, 153]            --\n",
       "│    │    └─Sequential: 3-4              [32, 256, 153]            295,936\n",
       "│    └─DoubleConvBlock: 2-5              [32, 512, 75]             --\n",
       "│    │    └─Sequential: 3-5              [32, 512, 75]             1,181,696\n",
       "├─AdaptiveAvgPool1d: 1-2                 [32, 512, 1]              --\n",
       "├─Linear: 1-3                            [32, 2]                   1,026\n",
       "==========================================================================================\n",
       "Total params: 1,575,074\n",
       "Trainable params: 1,575,074\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 7.32\n",
       "==========================================================================================\n",
       "Input size (MB): 0.96\n",
       "Forward/backward pass size (MB): 303.01\n",
       "Params size (MB): 6.30\n",
       "Estimated Total Size (MB): 310.27\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(32, 3, 2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataloaders['train']\n",
    "\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(batch[0].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seisbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
